\documentclass[12pt]{report}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage{csquotes}
\usepackage{algorithm}
\usepackage{algpseudocode}
\usepackage{graphics}
\usepackage{graphicx}
\usepackage[export]{adjustbox}
\usepackage{float}
\usepackage{amsthm}
\usepackage{amsmath}
\usepackage{listings}
\usepackage{hyperref}
\lstdefinestyle{mystyle}{    
    basicstyle=\footnotesize,
    numbers=left,
    stepnumber=1,
    showstringspaces=false,
    tabsize=1,
    breaklines=true,
    breakatwhitespace=false,
}

%Bibliography
\usepackage[style=numeric-comp,useprefix,hyperref,backend=bibtex]{biblatex}
\addbibresource{refs.bib}


\graphicspath{ {images/Final results/Architectures/Delays},{images/Final results/Architectures/Energy},{images/Final results/Architectures/Tasks}, {images} }
\usepackage{hyperref}
\usepackage{filecontents}
\usepackage{seqsplit}
\title{\centerline{\includegraphics[width=8cm,height=8cm]{logo.png}}
{\scshape Dipartimento di Informatica}\\
\vspace{0.5cm}
Orchestration and collocation of application for autonomous drive}
\author{Candidato: Tommaso Lencioni\\
Relatori: Prof. Patrizio Dazzi, Prof. Antonio Brogi}
\date{Anno Accademico: 2021/2022}

\begin{document}
\maketitle


\tableofcontents{}

\pagebreak

\paragraph*{Abstract}
This thesis tries to...


\part*{Introduction}
\chapter{Introduction to application allocation problem}
The growing interest in the cloud computing paradigm and its application in the automotive led research community to invest in this area (also with european project like \href{https://www.teaching-h2020.eu}{H2020 TEACHING}).

In this case particular attention is put on the creation of a distributed edge-oriented environment that integrates heterogeneous resources like edge devices, specialized on-site nodes and cloud.

\section{Application Placement}
The application placement problem is a concern for every large scale Fog infrastructure.

When the number of task generated and computational device rise the nearest/strongest offload location is not always the best.

This leads to the need of a methodology to find a suitable device for the task.


\section{State of the art}
The state of the art

\chapter{Scenario}
The scenario considered in this thesis is an intelligent vehicles environment. The way different components interact emulate a real world use although the application of some of the solutions described in the following sections are just for research purposes.

\section{Vehicles}
The users (vehicles) are represented as edge devices with no computational power that generate tasks in an interval of time.\\
They are mobile to simulate a realistic e behavior, this leads to strict delay and mobility requirements.

\subsection{Applications}
The proposed tasks are the following:
\begin{enumerate}
\item
	\begin{verbatim}
		SENSOR_DATA_COMMUNICATION
	\end{verbatim}
	Application that simulates the gathering and communication of data originated by vehicle's sensors.\\
	We take for granted a preliminary elaboration and anonymization of the data collected by the car done in respect of the privacy limits established with the end user.
	\item
	\begin{verbatim}
		ENTERTAINMENT
	\end{verbatim}
	Application that simulates the use of infotainment (music streaming, webradio, etc.)
\end{enumerate}

\section{Road site units}
Road side unit are devices that lay on the border of the road (possibly integrated in traffic light/sign).

Those data centers are simulated with edge data centers.

They represent a first point of arrival of the task sent by the devices.

The communication between the two happens through LAN and undergoes the limitation imposed by both edge devices's range and the edge data center's range.\\
They have computational power and can act as orchestrator.

This means that a device asks the data center to offload the task for it, extending in fact the range on which suitable computational power can be found at the expense of the introduction of some communication delay.\\
They don't generate tasks and they are the core actor in my \nameref{leader algo}.

\section{Cloud}
The Cloud is present as last resort due to high delay introduced in the offload on it.
It offer computational power and is reachable from every device through WAN without the physical limitation of the LAN.


\section{Thesis's goal}
The goal of this thesis is to analyze how the previously described system can be simulated with a simulation software (\ref{PES}) and to propose an algorithm that makes the edge data centers follow a hierarchy model.

\part*{Metodology}
\chapter{Pure Edge Sim}
\label{PES}
The tool used is \textbf{Pure Edge Simulator} (from here after \textit{PES}).

\textit{PES} is a simulation framework for performance evaluation developed by a group of Tunisian researchers focused on cloud, fog, and pure edge computing environments.

\section{Requirements}
The project is in Java and I used an IDE for my whole work.

This simplify the dependencies management but nullify the possibilities of scripting the tests.

\section{Features and limitations}
\textit{PES} is a task oriented simulator.

This trait is inherited from its parent tool Cloud Sim.\\
Although this approach to simulations is suitable for testing task scheduling on Cloud and Edge, such decoupling represents a major barrier for the implementation of consequentials events involving more than one architecture layer.
An insight of the structure can be take in the paper proposed by the authors \cite{MechalikhTM21}.

\section{Starting a simulation}
Starting from main in MainApplication the method launchSimulation is called.\\
Configuration files are parsed.\\
Scenarios (combinations of parameters) are loaded into Iterations.
The iterations are executed one by one in a for loop.\\
Before starting the simulation the models are loaded:
\begin{itemize}
	\item The data centers are generated (this includes the edge devices).
	\item The tasks are generated at random for the devices that can generate them (every device have an APPLICATION\_LIST).
	\item An orchestrator is set.
	\item A network model is set.
\end{itemize}
The method startInternal of SimulationManager is called.\\
Before scheduling the task as stated below there is a check for the flag ENABLE\_ORCHESTRATORS. If it is false the task's orchestrator is the generating device itself.
All the task are scheduled to "this" (the Simulation Manager) with a delay based on the time established during their generation and with the tag SEND\_TO\_ORCH.\\
Tasks regarding the simulation (log printing, charts updating and progress bar) are scheduled.\\
All the classes that can receive a scheduled task (not necessary an edge device's task) implements the method processEvent that evaluate the tag of the event in a switch, trying to match it to a known case.

\section{Task's life}
Every step in a task's life is handled by either Simulation Manager or Network Model.
The initial scheduling to itself done for every task as stated in the previous section causes the Simulation Manager to catch the tasks in the method \textbf{process event}.\\
Due to the SEND\_TO\_ORCH flag set before the method \textbf{sendTaskToOrchestrator} is called.
Check if the task is failed.
If the orchestrators are enabled the closest device among the non-cloud orchestrators is set as such for the task.
Then the task is scheduled immediately with \textbf{scheduleNow} to the Network Model with tag SEND\_REQUEST\_FROM\_DEVICE\_TO\_ORCH.\\
\\
As stated before the Network Model of the simulation receive the task and in its processEvent method the right case is caught, calling the method \textbf{sendRequestFromDeviceToOrch} right away.\\
If the orchestrator of the task is the generating device itself 
the task is immediately scheduled to the Simulation Manager with \seqsplit{SEND\_TASK\_FROM\_ORCH\_TO\_DESTINATION} tag.
Otherwise a new transfer with
\begin{itemize}
	\item file size = the number of bits of the task's file
	\item type = REQUEST
\end{itemize}
is added to \textbf{transferProgressList} in order to simulate the request traveling through the network from the source to the orchestrator. Let's consider this case.\\

\subsection*{Network transfers}
The \textbf{transferProgressList} is an ArrayList initialized at the creation of the Network Model object.

It is loaded with transfers objects to simulate any kind of displacement of information.

In the method startInternal of Network Model an event with tag UPDATE\_PROGRESS is scheduled and repeated every NETWORK\_UPDATE\_INTERVAL as we can see in processEvent.

Here updateTasksProgress is called every update cycle.

In this method transferProgressList is scanned and, for every transfer in queue, the number of transfers on the same network (both WAN and LAN) and their utilization are evaluated.
\begin{itemize}
	\item WAN is used when the transfer involves the Cloud.
	\item LAN is used when two transfers share an endpoint.
\end{itemize}
Those exchanges undergo the bandwidth limit that is set to the minimum between LAN and WAN.
The bandwidth is updated (updateBandwidth) as well as the transfer (updateTransfer).\\
In this part a crucial role is played by the flag REALISTIC\_NETWORK\_MODEL. In fact, if true, the remaining file size of the transfer is set after a subtraction between the old remaining file size and the NETWORK\_UPDATE\_INTERVAL times the current bandwidth. This implies that a simulation done in this way reflects how the network delay is impacting the performance of the whole system.\\
This is essential to point out because if the flag is false the remaining file size is simply set to 0. Considering the fact that this happens every NETWORK\_UPDATE\_INTERVAL, if the latter is set to a low value the simulation could be not faithful.\\
Upon finishing the transfer (therefore the remaining file size is 0) \textbf{transferFinished} is called.
Here the network usage is updated and the transfer is removed from the queue.
According to the type of the transfer (request, task, container, result to orchestrator or result to device) a different "if guard" is satisfied and the corresponding method is called. After that call the energy model is always updated\\
In our case (REQUEST as type) the method offloadingRequestRecievedByOrchestrator is called.

Here there is a check for the type of task's orchestrator:
\begin{itemize}
	\item If it's Cloud then the task is scheduled to Simulation Manager with a delay of WAN\_PROPAGATION\_DELAY and tag \seqsplit{SEND\_TASK\_FROM\_ORCH\_TO\_DESTINATION}
	\item otherwise same as above except the immediate schedule (no WAN propagation delay if the orchestrator is an Edge Device).
\end{itemize}
\vspace{0.5cm}
In Simulation Manager the corresponding switch case is caught in processEvent and sendFromOrchToDestination is called.
Here there is a check if the task is failed. After that the a suitable VM is searched for the task.

\subsection*{VM Choice}
The method initialize of the Simulation Manager's orchestrator is called. The architecture of the simulation is evaluated and the respective method is called. Whatever method is called the only difference is the array of strings Architecture (containing the names of the supported architectures in this simulation) passed to the method nesting of assignTaskToVM and findVM.

The latter is up to implementation and utilizes the simulation algorithm to find the suitable VM.\\
In my case I customized it in order to implement the \nameref{leader algo}.

I used negative integers returned from my\_initialize in order to distinguish the step of leader offload.

assignTaskToVM evaluates the VM (-1 results in a failure for lack of resources) and assign it to the task. 

%TODO fai capire che e' finita la sottosezione di VM Choice
\vspace{0.5cm}
There is a second evaluation for the VM=Null (this time in the Simulation Manager) then there is a check:
\begin{itemize}
	\item If the device that generated the task isn't the same that contains the assigned VM and the orchestrator of the task isn't the data center that contains the assigned VM then a task with tag \seqsplit{SEND\_REQUEST\_FROM\_ORCH\_TO\_DESTINATION} and destination the Network model is scheduled immediately.
	\item Otherwise the task is scheduled to this (Simulation Model) with tag EXECUTE\_TASK.
\end{itemize}
Let's consider the firts case.

In network model the corresponding case is caught and sendRequestFromOrchToDest is called.

A transfer is added to the queue with tag TASK. The transfer behavior is the same as mentioned in the Network transfer subsection. %TODO citalo.
Upon finishing the transfer the corresponding if is caught and executeTaskOrDownloadContainer is called.

Here there is a check for the ENABLE\_REGISTRY, in that case a new task with tag DOWNLOAD\_CONTAINER is scheduled.

Otherwise the task is scheduled to the Simulation Manager according to the type of data center of the VM assigned to the task (schedule with WAN\_PROPAGATION\_DELAY for Cloud and schedule immediately for the Edge) with the tag EXECUTE\_TASK.

In the simulation manager the corresponding branch is caught. There is a check for failure, then submitCloudlet is called on the broker (created at the initialization). The CPU utilization of the VM is increased accordingly and the energy model too.

The task is executed by the Cloud Sim part of the project.

At its termination (CLOUDLET\_RETURN case in processEvent of the DatacenterBrokerSimple class extension) the task is immediately scheduled to the simulation manager with tag TRANSFER\_RESULTS\_TO\_ORCH.

In Simulation Manager TRANSFER\_RESULTS\_TO\_ORCH case is taken. The CPU percentage is remove and the method sendResultsToOchestrator is called.

There is a check if the task is failed. Then:
\begin{itemize}
	\item If the source device isn't equal to the data center that contains the associated VM the task is scheduled immediately to the network model with tag SEND\_RESULT\_TO\_ORCH.
	\item otherwise the task is immediately scheduled to this (Simulation Manager) with the tag RESULT\_RETURN\_FINISHED.
\end{itemize}
Let's consider the first case.
In network model the corresponding case is caught and the method sendResultFromDevToOrch is called. Here there is a check:
\begin{itemize}
	\item if the orchestrator of the task isn't the device that generated the task then a file transfer is started with tag RESULTS\_TO\_ORCH
	\item otherwise the task is immediately scheduled to this (Network Model) with the tag SEND\_RESULT\_FROM\_ORCH\_TO\_DEV)
\end{itemize}
Let's consider the first case.

Upon finishing the transfer returnResultToDevice is called.
A check whether the orchestrator or the data center of the VM assigned to the task are Cloud. In that case the task with tag SEND\_RESULT\_FROM\_ORCH\_TO\_DEV is scheduled to this (Network Model) with WAN\_PROPAGATION\_DELAY.

Otherwise the same is done without delay.

The corresponding case is caught and the method sendResultFromOrchToDev is called. Here a new file transfer is added with tag RESULTS\_TO\_DEV.

Upon finishing the transfer the last else is caught and the method \mbox{resultsReturnedToDevice} is called. This schedules immediately the task for the Simulation Model with tag RESULT\_RETURN\_FINISHED.
\\
In Simulation Model the corresponding case i caught. A failure check is done, then a customizable method of the simulation orchestrator is called (resultsReturned) and the number of task is increased.

\section{Leader Algorithm}
\label{leader algo}
\paragraph*{Idea}
An edge data center should be able to elect a leader among the other edge data centers in reach if they meet some arbitrary conditions.

A leader (that can have a leader as well) can execute tasks on its VM or offload them to its subordinates.

As last chance the task is offloaded on the Cloud.

\subsection*{Implementation}

\subsubsection*{LeaderEdgeDevice class}
The custom class \textit{LeaderEdgeDevice} extends DefaultDataCenter.

In \textit{startInternal} a task with custom tag LEADER\_ELECTION is scheduled with INITIALIZATION\_TIME + 1 delay.

In \textit{processEvent} the tag is caught.

There is a check for:
 	\begin{itemize}
 		\item the device being an edge data center,
 		\item the device being an orchestrator.
 		
 		This allows data centers not flagged as orchestrators to not have a leader (although possibly being elected as such by other data centers, even that case is easily adjustable).
 		\item "LEADER" is the orchestration method,
  	\end{itemize}

If everything is true then the method \textbf{leader} is called.
  	\begin{itemize}
 		\item in a loop every datacenter is taken into account and is checked whether:
 		\begin{itemize}
 			\item it is not the same data center as the one evaluating,
 			\item it is an edge data center
 			\item the distance between the two data centers is smaller than the range of the edge data centers.
 		\end{itemize}
 		\item If it is the case then there is an evaluation about the MIPS (I used that criterion for electing a leader).
 		\begin{itemize}
 			\item If the MIPS of the candidate are greater than ones of the evaluator
 			\item and the max MIPS of the data center seen until that point is lower than the MIPS of the candidate
 		\end{itemize}

then the candidate becomes the (potentially temporary) leader, the max MIPS seen until that point is set to the new leader's MIPS.

Upon ending the loop if a leader has been found the current datacenter is added to its subordinates list and the flag isLeader is set to true (in case it has a leader as well).
 		
Otherwise the orchestrator flag is removed as well as the data center presence in Orchestrators List.
  	\end{itemize}
\subsubsection*{Leader algorithm}
\paragraph{Simulation Manager}

In Simulation Manager I modified the default behavior of the method sendFromOrchToDestination, hence where is decided which VM will execute the task.

I introduced 2 negative integer "error code" beside -1 ("no VM found").
\begin{itemize}
	\item -2: the task must be sent to the leader
	\item -3: the task must be sent to the cloud
\end{itemize}

Those codes are returned from the method my\_initialize.

A switch catch the correct corresponding case scheduling \seqsplit{SEND\_TASK\_FROM\_ORCH\_TO\_DESTINATION} adding a delay in case of scheduling to Cloud.\\
Due to the low flexibility of the tool I had also to specify the reason of failure distinguishing from failing for lack of resources and simply rescheduling the task.


\paragraph{LeaderEdgeOrchestrator}

In order to find a suitable VM using this hierarchy I created a simulation algorithm called LEADER.

I extended Orchestrator in order to implement a custom findVM method.

This implies to implement my\_initialize and the methods corresponding to the various architectures.

my\_findVM is called only in edgeAndCloud method because in other architecture there are less or more devices than expected.

If LEADER is used as simulation algorithm there is a check for the presence of both Cloud and Edge in the simulation architecture. If it is not the case an error is displayed and the simulation stops.

Otherwise the architecture is restricted to "Edge" and the method leader is called.\\

The algorithm simply iterates through hosts and VMs of the phase's devices in search for a VM that meet the condition imposed by the code.

The condition based on which a VM is selected is completely arbitrary.

I used one based on task's length, MIBS and latency.\\

This is the order of seek for a suitable VM is followed:
\begin{enumerate}
	\item phase 0: among the hosts of the orchestrator.
	\item phase 1: among the hosts of the leader of the original orchestrator and its subordinates
	
	There should not be a situation where the orchestrator has no leader and is chosen for the previous step so the presence of a leader should be certain at this phase.
\end{enumerate}

If the previous steps should result in no VM selected a try is done in phase 2 with the algorithm INCREASE\_LIFETIME (could have been any other algorithm) on the Cloud.\\

\paragraph{Phases}
Are determined by the type of the orchestrator.
Due to the fact that it changes in every step it's easy to tell in which phase the algorithm is in.
This is an escamotage used to bypass PES limitation in flow control of the tasks.\\

\subsection{Notes}

\paragraph{Static election condition} The leader election is done once at the start of the simulation because the conditions on which a device is elected as leader are statics.

This can be changed rescheduling the election with a delay in case of dynamics conditions.

\paragraph{Leader's omniscience}
In phase 1 there is a check for every host of every subordinate of the leader.

This implies an omniscience by the latter.

This is not realistic because the method findVM could discover a VM not owned by the orchestrator neither reliable to do in PES due to the continuous allocation of VM for tasks so this implementation is purely theoretical.

\paragraph{Chaining}
A case of chaining can occur when the data center A has B as leader but B in turn has C as leader.

Both A and B are orchestrator so they can both receive tasks.

During findVM, if a task has A as orchestrator and it doesn't meet the conditions, the task is initially offloaded to B and its subordinates.

If, again, no VM are found the task goes to C and its subordinates.

The cloud, as always, is the last resort.


\paragraph{Recursion}
A particular case of the situation above is when A = C.

This is a problem because there is no way to keep track of the devices that already checked for condition on a task.

This could results in a endless loop. (TOFIX)


\paragraph{Isolation}
If an edge data center orchestrator is isolated in such manner that it doesn't have both leader and subordinates it became useless.

Not having a leader elect it as such but it has no subordinates so it won't receive tasks due to the fact that is removed from the orchestrators list.

\paragraph{Parity}
In case of condition parity between data centers every device will act as leader, thus not being an orchestrator neither having subordinates.

This could lead to an empty orchestrators list.

\paragraph{Rejection}
In this implementation data center can't reject to be elected as leader. 

This will force it to execute tasks of its self proclaimed subordinates.

This behavior is easily fixable by placing a check for the value of a custom setting flag ("consent") before electing.


\chapter{Simulations and results}

\chapter{Conclusion}

\printbibliography
\end{document}